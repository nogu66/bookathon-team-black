= AI×仮説駆動学習のススメ — 第2回Bookathonで会いましょう

== はじめに

//flushright{

2026年2月
いとまど

//}
エンジニアとして働いていると、日々の業務の中で「自分はここが弱いな」と感じる瞬間があるはずです。基礎力の不足、学び方がわからない、なんとなく成長が止まっている気がする——そんな漠然とした課題を抱えながらも、日常の忙しさの中では向き合う機会がなかなかありません。
本書は、Bookathonというイベントを使って、そうした日頃の課題を解消した体験記です。私自身、「基礎がなってない」という課題を抱えてBookathonに参加し、メンターのアドバイスをきっかけに仮説駆動学習という学び方に出会いました。その結果、2日間で自分の課題に正面から向き合い、着実に前に進むことができました。
Bookathonは、技術的にすごい人たちだけが参加するイベントではありません。等身大の悩みを抱えた自分のような人間でも参加できますし、だからこそ得られるものがありました。あなたが抱えている課題が何であれ、Bookathonはそれに向き合う場になり得ます。本書を通じて、その可能性を感じてもらえたら幸いです。

=== 想定する読者層

本書は、こんな人に向けて書かれています。

 * 仕事や趣味でのエンジニアリング活動で、何らかの課題を抱えている人
 * 基礎力不足や学び方に悩んでいる若手エンジニア
 * 第2回Bookathonに興味があるけど、書きたいテーマがなくて応募が怖い人

=== 本書のゴール

 * Bookathoが自分の課題に向き合う場だと分かる
 * 第2回Bookathonに参加したくなる

//pagebreak

== Bookathonってもしかして最強の学習法では？

=== イベント駆動学習という見立て

応募する前に、私はこんな見立てを持っていました。@<b>{自分の特性的に、イベント駆動学習があっているのではないか？　Bookathonはそれを実践する最適な機会なのではないか？}Bookathon@<fn>{bookathon}というイベントなら、強制的にインプットとアウトプットをイベントの中で行うことで、効率よくレベルアップができるのではないかと考えたのです。メンターとして経験豊富なエンジニアの方々が参加しているので、学ぶべき技術のアドバイスももらえます。

//footnote[bookathon][技術書を書きたい人が集まり、3日間で原稿を完成させる短期集中執筆ハッカソンイベント。技巧社や翔泳社など大手出版社の編集者や著者がメンターとして参加する。]

=== Bookathonでの学び方の理想的な構造

ここで、私と似た属性の人だけでなく、仕事や趣味でのエンジニアリング活動で何らかの課題を抱えている人にも、Bookathonは効果があるのではないかと考えました。私のやり方をそのまま真似するのではなく、各々が抱えている課題に対して、@<img>{overall_bookathon}のような流れで取り組むのが理想的だと思っています。

//image[overall_bookathon][Bookathonでの学び方の理想的な構造]{
//}

この構造のポイントは、@<b>{自分なりに課題の解消方法を考えた上でメンターのアドバイスを聞く}ことです。自分の考えなしにアドバイスを聞くのと、自分なりに考えた上で聞くのとでは、得られる気づきの深さが違います。そして最終的に、取り組み自体とそこからの気づきを本にまとめることで、学びが定着します。具体例として、次の節で私自身の考えがどう変わっていったかを紹介していきます。

=== 応募前の私の考え

私は応募前に、自分なりの課題認識と、それに対する見立てを持っていました。

 * 課題：仕事が遅い・実力不足と感じる
 * 原因の見立て：基礎がなってないから。なぜ基礎がなってないかというと、基礎学習に面白さを見出せず疎かにしているから
 * 解決策の見立て：Bookathonという新しいイベントで、強制的にたくさんのインプット/アウトプットを行う企画にしてしまえば、楽しめるし頑張れるのではないか

課題と原因は正直に自覚していたつもりでした。解決策としては、「面白くないなら、面白い環境に身を置けばいい」というシンプルな発想でした。しかし、この考えはメンターのwakameさんから重要なフィードバックをもらうことになります。

=== wakameさんからのアドバイス

イベント初日、メンターのwakameさんから、私の考え方を変えるアドバイスをいただきました。以下は、私がwakameさんに前提として共有した文章です。

//image[before_menter][wakameさんに共有した前提の文章]{
//}

この文章をもとに、wakameさんから3つのアドバイスをいただきました。

==== ① 仮説を持って、手を動かして検証する

最も強調されたのは、仮説を持つことの重要性です。「基礎が足りないので、AIにおすすめされた通りに手を動かします！」というのは仮説ではありません。大事なのは、まず大きなテーマを決め、そのテーマの中で自分なりの具体的な問いを見つけ、その問いに対して仮説を立て、手を動かして検証するという流れです。テーマはあくまで探求の方向性であり、問いはその中で「自分が本当にわかっていないこと」を具体化したものです。できる先輩たちは仮説を立てて、それがあっている確率が高いそうです。最初から当たらなくても、@<b>{そもそも仮説を立てることで、スタートラインに立てる}のだと教えてもらいました。

==== ② 良い仮説を立てるには、知っている技術×少し知っているテーマを選ぶ

次に指摘されたのは、仮説を立てるための前提条件についてです。知らない技術×知らないテーマでは、そもそも仮説を持つことができません。3割くらい知っているテーマを、知っている技術で深掘る方が、自分なりの問いを見つけやすく、仮説も立てられるとのことでした。また、今回の2日間ではテーマは1〜2つに絞った方がいいともアドバイスをもらいました。量を追いすぎると浅くなってしまうからです。

==== ③ 失敗も成功もログに残す

最後のアドバイスは、検証の記録についてです。仮説が外れたら失敗、あっていたら成功として、どちらもログに残すことが大切だと言われました。あとで本を読んだときに、失敗した時のログが残っていると、本の知識と繋がりやすいし、モチベーションにもなるからです。

=== 仮説駆動学習を実践することにした

wakameさんのアドバイスを受けて、応募前の自分の考えを改めて振り返ってみました。課題の認識はあっていましたが、原因の深掘りが足りていなかったことに気づいたのです。

 * 原因の深掘り：なぜ基礎学習に面白さを感じないのか？それは、基礎学習として受動的に情報を受け取るものしか知らなかったからでした。本を読んだり、チュートリアルに書かれた内容を写経したり。今回のように、@<b>{問いを持って仮説を立てて検証するという能動的な学習}は、面白いと感じました
 * 解決策の見直し：自分は、困ったらパワー系の解決策に走ってしまい、実行しきれずに頓挫しがちでした。「仮説を立てて、実験して検証する」という行為はコスパが悪いものだと思っていました。実際にやってみて、確かに時間はかかりますが、得るものが大きかったです

この気づきを踏まえて、私の方針が固まりました。今回は、1つの仮説を手を動かして検証することにしました。使う技術はTypeScriptに絞ることにしました。

=== 自分ルール

さらに、自分なりのルールを設定しました。

==== ① 仮説を立てる際は、最初からAIを使わない

まずは自分で考えてみて、煮詰まったらAIに聞くようにしました。最初からAIに聞いてしまうと、問いを立てる力や、仮説を立てる力が付かないと思ったからです。実際にやってみて気づいたのですが、@<b>{AIを使わずに問いを立てて、仮説を立てようとすると、自分が何をわかっていないかが見えてきます。}「わかっていないこと」に気づけた。これだけでも大きな収穫でした。

==== ② 仮説を立てる以外の部分は、どんどんAIを使う

①とは対照的に、仮説を立てること以外の部分では、積極的にAIを活用しました。例えば、立てた仮説を実際のコードで検証する際や、文章の執筆などです。これは、限られた時間の中で、仮説を立てる部分に時間を効率的に投下するためです。

==== ③ 執筆時、ありのままを書くことを恥ずかしがらない

「こんな初歩的なことも知らないことがバレるのは恥ずかしい...」と自分を偽らないようにしました。恥ずかしい気持ちをあえて感じることで、これからの勉強への原動力にしたいと思ったからです。

//pagebreak

== 「失敗をいつ発見するか」というテーマに決めた

=== なぜこのテーマなのか

まず、テーマの候補をAIに出してもらいました。「技術やプロジェクトに共通して現れる、考え方の軸になるようなテーマを挙げてください」と聞いたところ、以下の5つが出てきました。

 * 複雑さをどこに押し込めているか
 * 失敗をいつ発見するか
 * 人間の判断をどこまで減らすか
 * 時間をどう扱うか
 * 責務の境界を誰が決めるか

この中で、「失敗をいつ発見するか」というテーマを選びました。理由は2つあります。

 1. テスト設計などを業務でやっている中で考える機会が多く、比較的考えやすいと思ったため
 2. 失敗はユーザーに直接迷惑がかかる分野であり、重大だと感じているため

=== 開発速度が遅いだけで、失敗はしていないのでは

新規プロダクトを開発しており、まだ私起因のエラーやインシデントは起きていません。開発速度が遅いだけで、失敗はしていないのでは？　しかしAIとの壁打ちの結果、そうではないとわかりました。以下の2つの理由によって、既に失敗していることが判明したのです。

 1. 実装後に動作確認してみたら意図した動作と違い手戻りが起きている：フロント側の状態管理が複雑になってきた影響で、ローカルで動作確認をしたら、意図した実装になっていませんでした。型エラーやビルドエラー、実行時エラーはなかったので、動作確認をして私が確認するまでわからなかったということになります
 2. 将来の外部失敗 or 開発速度が落ちる失敗の種を作っている可能性が高い：きっと、コードの複雑性を増やしてしまっています。また、良いテストも書けていない気がするため、偽陽性となる可能性もあります

=== 「失敗」を3段階で定義してみた

//image[fail_level][失敗の3段階]{
//}

=== テーマの中から問いを探す

テーマは決まりましたが、テーマだけでは仮説は立てられません。wakameさんのアドバイスの通り、テーマの中から具体的な問いを見つける必要があります。しかし、ここで手が止まってしまいました。AIのレビューで発見できる失敗とは何だろう？　そもそも、AIのレビューで発見できるものを仮説にしても、あまり意味がない気がします。AIに投げれば済む話だからです。では、AIのレビューで発見できないものとは何か？　考えてみましたが、うまく言語化できませんでした。そこで方向性を変えることにしました。まずは、AIのレビューで発見できるものを整理して、そこから深掘りしていけば、何か見えてくるかもしれません。最初に思いついたのが「型エラー」でした。しかし、ここでまた疑問が湧きます。型エラーは、AIを使わなくても型チェックで発見できます。VSCodeでもエラーは表示されます。では、それをわざわざ深掘りする理由は何なのか？　@<b>{この疑問こそが、問いを見つける入り口になりました。}

//pagebreak

== 1つ目の問い「型定義に穴があると、どの時点でエラーが起きるのか？」

=== テーマから問いへ：型エラーに絞ってみることにした

「失敗をいつ発見するか」というテーマの中から、具体的な問いを探していきます。前章で浮かんだ疑問「型チェックがあるのに、なぜ型を深掘りするのか？」から考え始めました。考え始めると、疑問がどんどん出てきました。

 * 型エラーって、正確には何を指すんだっけ？
 * 型エラーがないと、どの時点でどういうエラーが起きるんだっけ？
 * lint checkで発見できるんじゃないか？
 * VSCodeでエラー表示されるよね？
 * でも、テストファイルの型エラーをAIがいつまでも治せなかった覚えがある...なぜ？

考え始めると、どんどん疑問が出てきます。案外知らないことが多いと気づきました。当たり前に使っている型と型エラーに関して、今回は深掘ることにしました。いつも空気のように感じていた型というものは何なのか？型チェックが防止してくれるのに、それをあえて深掘る理由は何なのか？

=== 衝撃の事実：型エラーが出なくても不具合は起きる

煮詰まったので、AIに聞いてみました。AIに聞いた結果わかったのは、@<b>{型定義に穴があると、後々エラーとしても出ない不具合が起きる}ということです。正直、今まで型を蔑ろにしていました。型エラーが出なければOK、anyはダメくらいの気持ちだったのです。これまでAIが定義した型にちゃんと目を通してこなかったですし、どういう観点で型をチェックすればいいかも分かっていませんでした。

=== 問いの設定と実験シチュエーション

こうして、テーマ「失敗をいつ発見するか」の中から、具体的な問いが決まりました。問いは「@<b>{型定義に穴があると、どの時点でエラーが起きるのか？}」です。実験のシチュエーションは「ユーザー登録機能」に設定しました。バックエンドにデータをPOSTするというシンプルな機能なので、型定義の問題だけに集中できると考えたからです。では、この機能において、どんな型が考えられるでしょうか？　まず、自分なりにユーザー型を考えてみました：

//emlist[ユーザー型（自分で考えたもの）][typescript]{
User = {
  id: number,
  name: string,
  email: string,
  password: string
}
//}

次に、バックエンドに登録内容をポストする関数の型定義も考えてみました。ただし、返り値の型をどうすればいいか分からなかったので、仮置きで日本語を書いています：

//emlist[registerUser関数（返り値の型が分からない）][typescript]{
function registerUser(user: User): 成功したかしてないかの返り値の型 {

}
//}

これでミスが起きるかどうかも、この時点では分かりません。成功したかしていないかの返り値の型というのは、ライブラリでpostした結果のオブジェクトをそのまま入れるものにしようと思っています。一旦、返り値の型が分からないので、AIに聞いてみることにしました。AIに聞いた結果、Result型を用意した方がいいとのことでした。そうしないと、ライブラリを変えた時に面倒になるそうです。特定のライブラリに依存した型になってしまい、呼び出し元がライブラリの詳細を知る必要が出てくるとのことでした。

=== 仮説の設定

AIの回答を聞いて、「ライブラリを変えた時に面倒」とは言うものの、本当にそんなに面倒なのか？という疑問が湧きました。そこで、以下の仮説を立てました。

==== 仮説：ライブラリに依存した型定義にすると、他のライブラリに変えた時にめちゃめちゃ面倒くさくなる

これまでよく聞く話ですが、正直実感が湧きませんでした。どう面倒なのか具体的に見えていないので、自分の手で検証してみることにしました。

==== 仮説検証の手順

 1. ユーザー型と、Axiosに依存した型定義をした、post関数を用意する（バックエンドと実際に通信はしない）
 2. 動くことを確認する
 3. 他のライブラリに変えるとなったら、どのくらい大変？

今の予想としては、このくらいシンプルな実験なら、そこまで大変ではないのではないかと思っています。もし大変でなかったとしたら、今後どんな機能が追加されたら大変になるのか？と考えます。

=== 実験1: Axiosに依存した型定義を作ってみた

//note[この章以降の検証パートについて]{
ここからは、筆者が実際に手を動かして検証した内容が続きます。この本で大事なのは、どんな問いを立て、どんな仮説を持って検証に臨んだかという部分です。具体的な検証方法やコードの詳細は、そこまで重要ではありません。「こういう流れで検証しているんだな」という雰囲気だけ掴んでもらえれば十分です。読み飛ばしたい方は、「Bookathonハックで得た学び」の章まで飛んでいただければと思います。
//}

==== ステップ1・2：Axiosで実装して動作確認

まず、ユーザー型を用意します。

//list[types][types.ts][typescript]{
export type User = {
  id: number;
  name: string;
  email: string;
  password: string;
};
//}

次に、このユーザー型を使って、Axiosに依存した型定義の関数を作りました。ここが今回の実験のポイントです。

//list[user-service-axios][user-service-axios.ts][typescript]{
import axios, { AxiosResponse } from 'axios';
import { User } from './types.js';

export async function registerUser(user: User): Promise<AxiosResponse> {
  // 実際にはバックエンドに送信しないが、型定義の検証のためにコメントで残す
  // return await axios.post('https://api.example.com/users', user);

  return {
    data: { success: true, userId: user.id },
    status: 200,
    statusText: 'OK',
    headers: {},
    config: {
      headers: {} as any
    }
  } as AxiosResponse;
}
//}

//note[なぜモックレスポンスを使うのか]{
実際にAPI通信を行わない理由は2つあります。1つ目は、@<code>{example.com/users}はPOSTを受け付けていないためです。2つ目は、今回の検証は「型定義の依存性」に焦点を当てているため、実際の通信は本質的ではありません。むしろ、通信エラーやネットワークの問題など、本来の検証目的とは無関係な問題に時間を取られる可能性があります。

なお、実際にAPI通信を試したい場合は、@<href>{https://jsonplaceholder.typicode.com/users}のような公開テストAPIが利用できます。
//}

返り値の型が@<code>{Promise<AxiosResponse>}になっています。これが特定のライブラリに依存した型定義です。動作確認のコードは以下の通りでした。
//list[main][main.ts][typescript]{
import { registerUser } from './user-service-axios.js';
import { User } from './types.js';

async function experiment1() {
  const testUser: User = {
    id: 1,
    name: '山田太郎',
    email: 'yamada@example.com',
    password: 'password123'
  };

  const response = await registerUser(testUser);

  console.log('ステータスコード:', response.status);
  console.log('ステータステキスト:', response.statusText);
  console.log('レスポンスデータ:', response.data);

  if (response.status === 200) {
    console.log('✅ ユーザー登録成功!');
  }
}

experiment1();
//}

実行してみると、以下のような出力が得られました：

//cmd{
ステータスコード: 200
ステータステキスト: OK
レスポンスデータ: { success: true, userId: 1 }
✅ ユーザー登録成功!
//}

ちゃんと動きました。しかし、ここで問題に気づきます。呼び出し元のコードを見ると、@<code>{response.status}、@<code>{response.statusText}、@<code>{response.data}というAxiosの型に依存したプロパティに直接アクセスしています。

==== ステップ3：fetchに変更してみたら...

実際にライブラリを変更するとどのくらい大変なのか、試してみることにしました。AxiosからfetchのResponse型に変更してみます。

//list[user-service-fetch][user-service.ts][typescript]{
import { User } from './types.js';

export async function registerUser(user: User): Promise<Response> {
  // 実際にはバックエンドに送信しないが、型定義の検証のためにコメントで残す
  // return await fetch('https://api.example.com/users', {
  //   method: 'POST',
  //   headers: { 'Content-Type': 'application/json' },
  //   body: JSON.stringify(user),
  // });

  return new Response(
    JSON.stringify({ success: true, userId: user.id }),
    {
      status: 200,
      statusText: 'OK',
      headers: { 'Content-Type': 'application/json' }
    }
  );
}
//}

すると、呼び出し元のコードも変更が必要になりました。

//list[main-fetch][main.ts（fetch版）][typescript]{
import { registerUser } from './user-service.js';
import { User } from './types.js';

async function experiment1() {
  const testUser: User = {
    id: 1,
    name: '山田太郎',
    email: 'yamada@example.com',
    password: 'password123'
  };

  const response = await registerUser(testUser);

  console.log('ステータスコード:', response.status);
  console.log('ステータステキスト:', response.statusText);

  // ⚠️ Axiosと違い、dataプロパティは存在しない
  // レスポンスボディを明示的にパースする必要がある
  const data = await response.json();
  console.log('レスポンスデータ:', data);

  if (response.status === 200) {
    console.log('✅ ユーザー登録成功!');
  }
}

experiment1();
//}

呼び出し元のコードを変更する必要が出てきました。Axiosでは@<code>{response.data}で直接JSONオブジェクトにアクセスできましたが、fetchでは@<code>{await response.json()}で明示的にパースが必要です。これは良くありません。なぜなら、呼び出し元がライブラリのレスポンスのデータ構造を知っておかなければならないからです。また、ライブラリのアップデートやバージョン変更などで構造が変わった場合にも、呼び出し元のコードを変更しなければならないというリスクがあります。

=== ここまでの実験で感じたこと

型エラーが出ないからといって、良い型定義とは限りません。返り値の型が特定のライブラリに依存していると、将来の変更が困難になります。実際にコードを書いて動かすことで、その意味が腹落ちしました。@<b>{知識として知っていることと、実感として理解することは違います。}ただ、正直「一行変えるだけでは？」とも思いました。@<code>{response.data}を@<code>{await response.json()}に変えるだけですし、呼び出し箇所が多くても全部置換すればいいのではないか、と。本当に単純な置換で済まないケースはあるのでしょうか？

=== 「置き換えればいいじゃん」が通用しないケースを試してみた

「全部置換すればいいのでは？」という疑問を持ったので、実際に検証してみることにしました。AIに「コード追加は最小限で、なおかつ『置き換えればいい』が通用しないケースを作ってください」と指示して、コードを追加してもらいました。以下のコードは、Axiosの@<code>{response.data}を@<code>{await response.json()}に単純置換した結果、エラーが起きることを示すための実験コードです。意図的にエラーが出るように書いてあります。

//list[main-patterns][main.ts（fetchに変更後）][typescript]{
import { registerUser } from './user-service.js';
import { User } from './types.js';

async function main() {
  const testUser: User = {
    id: 1,
    name: '山田太郎',
    email: 'yamada@example.com',
    password: 'password123'
  };

  const response = await registerUser(testUser);

  // パターン1: 単純なdata参照
  console.log('1. レスポンスデータ:', await response.json());

  // パターン2: dataの中のプロパティを直接参照
  console.log('2. ユーザーID:', await response.json().userId);

  // パターン3: 条件分岐でdataを使用
  if (await response.json().success) {
    console.log('3. 成功フラグ確認: OK');
  }

  // パターン4: dataを変数に格納して使用
  const result = await response.json();
  console.log('4. 変数経由でアクセス:', result.userId);

  // パターン5: 分割代入
  const { success, userId } = await response.json();
  console.log('5. 分割代入:', success, userId);
}

main();
//}

実際にファイルを開いたら、型エラーがたくさん出ていました。エラーメッセージは以下の通りです：

//emlist[][]{
プロパティ 'userId' は型 'Promise<unknown>' に存在しません。
プロパティ 'success' は型 'Promise<unknown>' に存在しません。
'result' は 'unknown' 型です。
プロパティ 'success' は型 'unknown' に存在しません。
プロパティ 'userId' は型 'unknown' に存在しません。
//}

なぜエラーが出ているか分からなかったので、AIに聞いてみました。すると、Axiosとfetchでレスポンスのデータ型が根本的に異なることが分かりました。

 * Axiosの@<code>{response.data}: JSONにパースされたオブジェクト
 * fetchの@<code>{response.json()}: 生データをJSONにパースするメソッド

さらに、fetchはストリームという仕組みを使っているため、一度生データからJSONに変換したら、生データは消えてしまうそうです。メモリを節約するために、そういう設計になっているとのことでした。「JSONにパースするだけでしょ」と思っていたのですが、中身のデータ型が全然違っていたのです。だから、上記のコードでは@<code>{response.json()}を何度も呼んでいますが、2回目以降はエラーになります。一度パースしたら、もう生データは残っていないからです。とはいえ、データ量が多いときには助かる設計なので、使い分けが大事だと思いました。つまり、@<code>{response.data}を@<code>{await response.json()}に単純置換すると、以下の問題が起きます：

 1. 何度もアクセスしている箇所で、2回目以降がエラーになる
 2. 変数に入れるタイミングを考え直す必要がある
 3. 各パターンごとに個別対応が必要

そんなことを呼び出し側がいちいち考えるのは現実的ではありません。ライブラリに依存した型を返り値に設定すべきではないと実感しました。さらに、他のHTTPライブラリの返り値の型を調べてみました。

//table[lib-comparison][HTTPライブラリごとのレスポンス型の違い]{
.	ステータス取得	データ取得	データの型
-------------------------------------------------------------
Axios	@<code>{response.status}	@<code>{response.data}	パース済みオブジェクト
fetch	@<code>{response.status}	@<code>{await response.json()}	ストリーム→パース（1回限り）
got	@<code>{response.statusCode}	@<code>{response.body}	文字列
//}

ライブラリが違えば、プロパティ名もデータの取得方法もこれだけ変わります。呼び出し側がライブラリの詳細を知らなくても使えるようにする必要があると、改めて実感しました。

=== 問いに対する答え

今回の検証で分かったのは、型エラーとしては起きないということです。TypeScriptの型チェックは通ります。しかし、変更を加えた時点で問題が顕在化します。具体的には以下の通りでした。

 * ライブラリ変更時に、呼び出し側すべての修正が必要になる
 ** 前節の実験で見たように、単純置換では済まない修正が必要
 * 型エラーは出ないため、実行して初めて気づくケースもある
 ** @<code>{response.json()}を複数回呼ぶとエラーになるが、型チェックでは検出されない
 * 影響範囲が広いほど、修正コストが大きくなる
 ** 呼び出し箇所が増えるほど、手動での確認・修正作業が増える

つまり「型定義に穴がある」とは、型チェックをすり抜けて、将来の変更を困難にする設計上の問題を含んでいる状態のことでした。今回の場合、@<code>{Promise<AxiosResponse>}という型定義は文法的には正しく、型チェックも通ります。しかし、ライブラリの実装の詳細に依存しているという「穴」があったのです。この穴は、型システムでは検出できません。だからこそ、@<b>{型定義を書く時点で、将来の変更を見据えた設計が必要}なのだと実感しました。

=== AIに聞いてみた：「これを抽象化したらどうなりますか？」

ここまで学んだ後、ふと疑問が湧きました。今回学んだ「ライブラリの型を返り値にしない」という知識は、他の場面でも使えるのでしょうか？「これを抽象化したらどうなりますか？」とAIに聞いてみました。すると、AIがより汎用的な判断基準を教えてくれました：

 1. これは境界（外部と内部の接点）か？
 ** 境界なら、外部の型を内部の型に変換しているか？
 2. これは置き換え可能か？
 ** 実装を変えたとき、この型定義も変える必要があるか？
 3. これはテストしやすいか？
 ** モックを書くのが面倒か？

1つでも「問題あり」なら、型定義を見直す必要があるとのことでした。本質は「実装の詳細が、インターフェースに漏れ出していないか？」ということです。今回の場合：

 * 実装の詳細：Axiosを使っている
 * インターフェース：registerUser関数の型定義
 * 漏れ出し：@<code>{Promise<AxiosResponse>}という返り値の型

実装の詳細（Axios）が、インターフェース（型定義）に漏れ出していたのが問題だったのです。AIは「依存性逆転の原則（SOLID原則のD）に違反している」とも教えてくれました。これまでSOLID原則は何度も聞いたことがありましたが、暗記のようで定着しませんでした。しかし今回の実験を通して、@<b>{「依存性逆転とはこういうことか」と実感として理解}できました。理想の依存関係と、今回の問題のある依存関係を比べてみましょう。

//emlist[理想：抽象を介して依存する]{
┌──────────────────────┐
│   呼び出し側（上位）    │
└─────────┬────────────┘
          │ 依存
          ▼
┌──────────────────────┐
│  抽象的な型            │
│  （Result<T, E>）      │
└─────────┬────────────┘
          │ 依存
          ▼
┌──────────────────────┐
│  実装（Axios, fetch）  │
└──────────────────────┘
//}

//emlist[問題（今回のケース）：具体に直接依存している]{
┌──────────────────────┐
│   呼び出し側（上位）    │
└─────────┬────────────┘
          │ 依存
          ▼
┌──────────────────────┐
│  Promise<AxiosResponse>│ ← 具体的なライブラリの型！
│  （具体的な型）         │
└──────────────────────┘
          ↑ 逆向きの依存
┌──────────────────────┐
│  実装（Axios）         │
└──────────────────────┘
//}

抽象が具象に依存してしまっていたのです。「テストしやすいか？」という観点も腹落ちしました。実装の具体的な詳細を型に反映してしまっていると、その具体に関してうまくモックできません。

//list[mock-comparison][モックの複雑さの違い][typescript]{
// ❌ モックが複雑（Axiosの詳細を知る必要がある）
const mockResponse: AxiosResponse = {
  data: { id: 1 },
  status: 200,
  statusText: 'OK',
  headers: {},
  config: { headers: {} as any }
};

// ✅ モックが簡単（ドメインの知識だけでOK）
const mockResult: Result<User, Error> = {
  success: true,
  data: { id: 1, name: 'test' }
};
//}

「@<code>{Promise<AxiosResponse>}はダメ」と覚えるのではなく、「なぜダメなのか」「どう判断すればいいのか」という考え方を手に入れることが大事だと気づきました。これなら、次に別のライブラリを使うときも、データベースを使うときも、同じ考え方で判断できます。@<b>{知識として知っていたSOLID原則が、仮説を立てて検証することで実感として理解できた。}これが今回の一番の収穫でした。

//pagebreak

== Bookathonハックで得た学び

=== 仮説駆動学習の効果

wakameさんの言葉通り、「仮説を立てることでスタートラインに立つ」という経験をしました。自分で仮説を立てて、失敗して、その過程を記録しておきました。これから技術書を読むときに、「あ、これBookathonで失敗したやつだ！」と繋がる瞬間が楽しみです。

特に印象的だったのは、@<b>{AIを使わずに問いを立てて、仮説を立てようとするプロセスそのものが、自分の理解度を映す鏡になった}ということです。型エラーについて考え始めたとき、案外知らないことが多いと気づきましたが、あれこそがまさに仮説駆動学習の効果でした。AIに最初から聞いていたら、「そうなんだ」で終わっていたはずです。自分で考えたからこそ、「わかっていないこと」に気づけました。

=== 他の学び方との比較

Bookathonを経験して、他の学び方との違いが見えてきたので、紹介します。

==== ハッカソンとの違い

 * 自分のペースで学習できる：ハッカソンはチームの進捗に合わせる必要があるが、Bookathonは自分の課題に集中できる
 * 自分自身の課題に向き合える：ハッカソンは「何を作るか」が主題だが、Bookathonは「自分が何を理解していないか」に向き合える
 * アウトプットの形が言語化：言語化するからこそ、理解や考えが甘い部分が如実に出る。ハッカソンは、それっぽいものが作れれば誤魔化せる時もある

==== 家での学習との違い

 * 強制力がある：締め切りと周りの参加者がいることで、サボれない環境ができる
 * すぐ聞けるメンターがいる：家で一人で学習していたら出会えないレベルのメンターに、その場で質問できる

==== 会社でメンターに教えてもらうこととの違い

日頃の業務や1on1でも、変えた方が良い点に気づく機会はあります。しかし、それをすぐに実行できるわけではありません。その時は「やろう」と思っても、実行できる機会には気持ちが半減していたりするものです。@<b>{Bookathonは、気づいたことをその場で即座に試せる環境}でした。

==== Bookathonのデメリット

一方で、執筆活動のコストは大きいです。学習だけに集中したい人には向かないかもしれませんね。

==== 技術書執筆への門戸が開かれる

これは個人的に大きいと思っています。Bookathonに参加することで、読む側から書く側に変わるきっかけになります。技術書の執筆は敷居が高いと感じている人にとって、メンターのサポートを受けながら書き始められる環境は貴重だと感じました。

=== 仮説駆動学習を日常に落とし込むには

今回、仮説を立てて検証する重要性はよくわかりました。ただ正直なところ、まだ明日から日常的にできる実感は湧いていません。Bookathonのような強制力がなくても続けられるようにするには、どうすればいいのでしょうか？

==== 「面倒くささ」の正体を分解してみた

まず、なぜ面倒に感じるのかを考えてみました。分解すると、3つの要素がありました。

 1. 慣れの問題：仮説を立てるときの頭の使い方がまだ身についていない。どう考え始めればいいかわからない
 2. 心理的な壁：間違った仮説を立てることへの恥ずかしさ。自覚しにくいが、無意識に避けてしまっている
 3. 強制力の欠如：Bookathon以外の日常で、やらざるを得ない仕組みがない

==== Bookathonと日常の決定的な違い

Bookathonでは「テーマを決めて、問いを立てて、仮説を立てて...」という丁寧なプロセスが成立しました。しかしそれは、2日間まるごとそれに使えるという特殊な環境があったからです。日常の業務では「さあ今から仮説を立てるぞ」という時間は来ません。来るのは、PRレビューで「なんかモヤっとする」瞬間、エラーが出て「なんでだろう」と思う瞬間、AIの提案を見て「本当にこれでいいのか？」と感じる瞬間です。つまり、@<b>{仮説駆動学習は「始めるもの」ではなく、「気づくもの」}なのだと思います。

==== 「たぶん〜だろう」は、すでに仮説だった

ここまで考えて、ひとつ大きな気づきがありました。「たぶん大丈夫だろう」「たぶんこれで動くだろう」——こういった「たぶん〜だろう」という思考は、実は日常の中で何十回も無意識にやっていることです。そしてそれは、@<b>{すでに仮説}です。今までは「たぶん〜だろう」で素通りしていました。しかし、その瞬間に「本当に？」と一度立ち止まること。これが、仮説駆動学習の最小単位なのだと気づきました。

==== 面倒くささへの処方箋

3つの面倒くささに対して、今回の経験から見えたヒントがあります。

===== 慣れの問題：「これって正確には何だっけ？」から始める

今回の検証の中で、「型チェックがあるのに、なぜ型を深掘りするのか？」から問いが連鎖的に出てきた経験をしました。最初の一歩は、当たり前に使っているものを選んで「これって正確には何だっけ？」と聞くことでした。そこから先は自然と疑問が出てきます。

===== 心理的な壁：間違った仮説こそ、学びが大きかった

「このくらいシンプルなら大変じゃないのでは？」という仮説が外れたことで、response.json()のストリームの仕組みやSOLID原則の実感的理解に繋がりました。@<b>{間違った仮説こそ、学びが大きかった}のです。完璧な仮説を立てる必要はありません。「〜だと思う」と言えれば、それで十分です。

===== 強制力の欠如：「驚いたとき」をトリガーにする

ログを残す仕組みを用意しておくのが有効だと考えています。ただし、Bookathonレベルの丁寧なログを毎日書くのは現実的ではありません。「〜だと思ったけど、〜だった」の一文だけでいい。タイムズチャンネルやObsidianなど、すでに使っている場所に書く。そして、「仮説を立てたとき」ではなく@<b>{「驚いたとき」をトリガーにする}。驚きは「自分の予想と現実がズレた瞬間」なので、それ自体が仮説検証の結果です。

==== 仮説駆動学習は、新しい習慣ではなかった

Bookathonで経験した「テーマ→問い→仮説→検証→ログ」は、丁寧にやるとあの形になります。しかし最小形は「たぶん〜だろう→確認した→違った/合ってた」の3ステップです。仮説駆動学習を日常に落とし込むとは、新しい習慣を作ることではなく、@<b>{すでにやっていることに名前をつけて、意識すること}でした。「たぶん〜だろう」と思った瞬間に、「本当に？」と一度だけ立ち止まる。それだけで、仮説駆動学習は始まっています。

//pagebreak

== おわりに: 書きたいテーマがなくて応募が怖いあなたへ

「書きたいテーマがない」「技術力が足りない」「他の参加者はすごい人ばかりなのでは」――私も同じ不安を抱えていました。でも、Bookathonは、完璧な知識がなくても、テーマが明確でなくても、参加できるイベントでした。メンターにアドバイスをもらい、仮説を立てて、手を動かし、失敗も成功もログに残し、ありのままを書く。@<b>{この仮説駆動学習のサイクルを回すことで、2日間で着実に成長できました。}この記事を読んで、少しでも「応募してみようかな」と思ってくれたら嬉しいです。第2回Bookathonで、あなたに会えることを楽しみにしています。

